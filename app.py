import gradio as gr
from transcribe import stream_transcribe  # æ‚¨è‡ªå®šä¹‰çš„æ¨¡å—
from PIL import Image, ImageDraw, ImageFont  # Pillow library
import io, re 
import numpy as np  # ç”¨äºåˆ›å»ºç¤ºä¾‹å›¾åƒ
import os  # ç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„å’Œç›®å½•
import shutil  # ç”¨äºæ–‡ä»¶å¤åˆ¶
import uuid  # ç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
from typing import List, Tuple, Optional, Union  # For type hinting
from llama_index.core.tools import FunctionTool
from llama_index.core.agent import ReActAgent
from tools.test import test_tool
from llm import DashScopeLLM
import json


# --- é…ç½®ä¿å­˜ä¸Šä¼ å›¾ç‰‡çš„ç›®å½• ---
UPLOAD_IMAGE_DIR = "user_uploaded_images"
os.makedirs(UPLOAD_IMAGE_DIR, exist_ok=True) # ç¨‹åºå¯åŠ¨æ—¶å³åˆ›å»ºç›®å½•

# --- 0. ä¿®æ”¹åçš„ Agent å‡½æ•° (ç°åœ¨æ¥æ”¶å›¾åƒè·¯å¾„åˆ—è¡¨) ---
def multimodal_agent_logic(
    image_path: Optional[str],  # Agent æ¥æ”¶å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    text_from_speech: Optional[str]
) -> Tuple[Optional[Union[Image.Image, str]], Optional[str]]:
    print(image_path)
    print(text_from_speech)
    """
    æ¨¡æ‹Ÿå¤šæ¨¡æ€ Agentï¼Œæ¥æ”¶å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨å’Œè¯­éŸ³æ–‡æœ¬ï¼Œ
    å¹¶åŠ¨æ€å†³å®šè¾“å‡ºå›¾åƒã€æ–‡æœ¬æˆ–ä¸¤è€…ã€‚

    Args:
        list_of_image_paths (Optional[List[str]]): ä¿å­˜åˆ°æœ¬åœ°çš„å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
        text_from_speech (str): ä»è¯­éŸ³è½¬å½•ä¸­æå–çš„æ–‡æœ¬ã€‚

    Returns:
        tuple: (image_data_to_display, text_data_to_display)
               image_data_to_displayå¯ä»¥æ˜¯ PILå›¾åƒå¯¹è±¡ã€æ–°å›¾åƒæ–‡ä»¶è·¯å¾„æˆ–Noneã€‚
               text_data_to_displayå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–Noneã€‚
    """
    agent_image_output: Optional[Union[Image.Image, str]] = None
    agent_text_output: Optional[str] = None

    try:
        llm = DashScopeLLM()
    except Exception as e:
        print(f"è­¦å‘Š: DashScopeLLM åˆå§‹åŒ–å¤±è´¥: {e}")

    optic_nerve_tool = FunctionTool.from_defaults(fn=test_tool)
    try:
        agent = ReActAgent.from_tools([optic_nerve_tool], llm=llm, verbose=True)
    except Exception as e:
        print(f"é”™è¯¯: ReActAgent åˆå§‹åŒ–å¤±è´¥: {e}")
        agent = None
        
    input_query = 'ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦å½±åƒè¯­éŸ³æŠ¥å‘Šç³»ç»Ÿï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šç»™ä½ æä¾›ä¸€æ®µéŸ³é¢‘è¯­éŸ³è¯†åˆ«çš„å†…å®¹ï¼ˆåŒ»å­¦å†…å®¹ï¼‰å’Œè¾“å…¥å›¾åƒ(å¦‚æœæœ‰)ï¼Œç”±äºè¯­éŸ³è¯†åˆ«ç²¾åº¦é—®é¢˜ä½ éœ€è¦è‡ªå·±ç†è§£å†…å®¹å¹¶ç»™å‡ºåˆé€‚çš„å›å¤ã€‚å¯¹äºæŠ¥å‘Šç”Ÿæˆè¦æ±‚ï¼Œå·¥å…·è°ƒç”¨ä¼šå¯èƒ½ä¼šæä¾›æ£€ç´¢å‡ºç›¸å…³æ¨¡æ¿ï¼Œè¯·ä½ æŒ‰ç…§æœ€ç›¸å…³çš„æ¨¡æ¿å†…å®¹å›ç­”ï¼Œå›ç­”é£æ ¼ç²¾å‡†ç²¾ç‚¼åƒä¸€ä¸ªå·¥å…·ã€‚è¯­éŸ³è¯†åˆ«ç»“æœï¼š'+text_from_speech
    if image_path:
        input_query += f"å›¾åƒè·¯å¾„:'{image_path}'"
    llm = DashScopeLLM()
    
    tools = [
        {
            'type':'function',
            'function':{
                'name': 'test_tool',
                'description': """
                                å¤„ç†çœ¼åº•å›¾åƒä»¥æ‰§è¡ŒæŒ‡ç¤ºæ€§çš„è§†ç¥ç»æµ‹é‡ï¼Œå¹¶æ£€ç´¢å‡ºç›¸å…³çš„æŠ¥å‘Šå†…å®¹ã€‚
                                æ­¤å·¥å…·åŠ è½½ç”± 'image_path' æŒ‡å®šçš„å›¾åƒæ–‡ä»¶ï¼Œåœ¨ä¸è§†ç›˜åˆ†æç›¸å…³çš„ç‰¹å®šåæ ‡
                                ï¼ˆä¾‹å¦‚ (165,423), (240,423), (202,353)ï¼‰ä¸Šå åŠ é¢„å®šä¹‰çš„ç»¿è‰²åå­—æ ‡è®°ï¼Œ
                                å¹¶æ·»åŠ ç»¿è‰²æ–‡æœ¬æ³¨é‡Šï¼ŒæŒ‡ç¤ºä¸€ä¸ªè®¡ç®—å‡ºçš„åº¦é‡å€¼ï¼ˆä¾‹å¦‚ "è§†ç›˜åº¦é‡: 1.09 å•ä½"ï¼‰ã€‚
                                å®ƒä¸»è¦ç”¨äºä¸è§†ç¥ç»ä¹³å¤´ç‰¹å¾ç›¸å…³çš„åˆæ­¥è§†è§‰è¯„ä¼°æˆ–è‡ªåŠ¨åŒ–åˆ†ææµç¨‹ã€‚
                                """,
                'parameters': {
                    "type": "object",
                    "properties": {
                        "image_path": {
                        "title": "Image Path",
                        "description": "çœ¼åº•å›¾åƒçš„æ–‡ä»¶è·¯å¾„ã€‚å›¾åƒæ–‡ä»¶åº”èƒ½è¢« OpenCV è¯»å–ã€‚",
                        "anyOf": [{"type": "string" },{"type": "null"}]
                        }
                    },
                    "required": ["image_path"]
                }
            }
        }
    ]
    
    function_mapper = {
        "test_tool": test_tool
    }
    messages = [
        {
            "role": "user", 
            "content": f"{input_query}"
        }]
    response = llm.chat(messages=messages, tools=tools)
    print(response.raw)
    function_output = None
    if response.raw['choices'][0]['message'].get('tool_calls', None):
        func = response.raw['choices'][0]['message']['tool_calls'][0]
        function_output = function_mapper[func['function']['name']](**json.loads(func['function']['arguments']))
        function_output = function_output if function_output else ''
        messages.append(response.raw['choices'][0]['message'])
        messages.append({"role": "tool", "content": function_output, "tool_call_id": func['id']})
        response = llm.chat(messages=messages)
        print(response.raw)
    if function_output:
        match = re.search(r"å·²ä¿å­˜è‡³'(.*?)'ã€‚", function_output)
        if match:
            agent_image_output =  match.group(1)

    agent_text_output = str(response)
    return agent_image_output, agent_text_output


# 1. å®šä¹‰æ ¸å¿ƒå¤„ç†å‡½æ•° (ä¿®æ”¹åä»¥ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°å¹¶ä¼ é€’è·¯å¾„)
def process_inputs_for_dynamic_agent_output(
    temp_image_path: Optional[str],  # Gradioä¼ å…¥çš„ä¸´æ—¶å›¾ç‰‡è·¯å¾„åˆ—è¡¨
    audio_input_path: Optional[str]
) -> Tuple[Optional[Union[Image.Image, str]], Optional[str]]:
    """
    æ¥æ”¶Gradioçš„ä¸´æ—¶å›¾ç‰‡è·¯å¾„ï¼Œå°†å…¶ä¿å­˜åˆ°æœ¬åœ°æŒ‡å®šæ–‡ä»¶å¤¹ï¼Œ
    ç„¶åå°†è¿™äº›æ–°è·¯å¾„ä¼ é€’ç»™Agentã€‚
    """
    saved_image_paths_for_agent: List[str] = []

    if temp_image_path:
        print(f"æ”¶åˆ°{temp_image_path}")
        try:
            # å°è¯•è·å–åŸå§‹æ–‡ä»¶æ‰©å±•åï¼Œæˆ–ä»å†…å®¹åˆ¤æ–­
            pil_img_temp = Image.open(temp_image_path)
            original_format = pil_img_temp.format
            pil_img_temp.close() # æ‰“å¼€ä»…ä¸ºè·å–æ ¼å¼ï¼Œç„¶åå…³é—­

            if original_format:
                extension = f".{original_format.lower()}"
            else: # å¦‚æœPILæ— æ³•ç¡®å®šæ ¼å¼ï¼Œå°è¯•ä»è·¯å¾„è·å–æˆ–é»˜è®¤
                _, ext_from_path = os.path.splitext(temp_image_path)
                extension = ext_from_path if ext_from_path else ".png" # é»˜è®¤.png

            # åˆ›å»ºä¸€ä¸ªæ–°çš„å”¯ä¸€æ–‡ä»¶åå¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•
            unique_filename = f"{uuid.uuid4()}{extension}"
            destination_path = os.path.join(UPLOAD_IMAGE_DIR, unique_filename)
            
            shutil.copy(temp_image_path, destination_path) # ä»ä¸´æ—¶è·¯å¾„å¤åˆ¶åˆ°ç›®æ ‡è·¯å¾„
            saved_image_paths_for_agent.append(destination_path)
            print(f"å›¾ç‰‡å·²ä»Gradioä¸´æ—¶ä½ç½®å¤åˆ¶å¹¶ä¿å­˜åˆ°: {destination_path}")
        except Exception as e:
            print(f"å¤„ç†/ä¿å­˜æ¥è‡ª {temp_image_path} çš„å›¾ç‰‡æ—¶å‡ºé”™: {e}")

    else:
        print("æ²¡æœ‰å›¾ç‰‡è¢«ä¸Šä¼ æˆ–Gradioæœªæä¾›ä¸´æ—¶è·¯å¾„ã€‚")

    # --- å¤„ç†è¯­éŸ³è¾“å…¥ (é€»è¾‘ä¸ä¹‹å‰ç›¸åŒ) ---
    transcribed_text = "è¯­éŸ³æœªè¯†åˆ«"
    if audio_input_path is not None:
        print(f"å°è¯•è½¬å½•éŸ³é¢‘: {audio_input_path}")
        try:
            transcribed_text_result = stream_transcribe(audio_input_path)
            if transcribed_text_result is not None and transcribed_text_result.strip():
                transcribed_text = transcribed_text_result
            else:
                transcribed_text = "è¯­éŸ³è¯†åˆ«ä¸ºç©ºæˆ–å¤±è´¥"
            print(f"è¯­éŸ³è¯†åˆ«ç»“æœ: {transcribed_text}")
        except Exception as e:
            print(f"è°ƒç”¨ stream_transcribe æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            transcribed_text = f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {str(e)}"
    else:
        transcribed_text = "æœªæ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥"
        print("No audio input detected.")

    # --- è°ƒç”¨ Agent, ä¼ é€’ä¿å­˜åçš„å›¾åƒè·¯å¾„åˆ—è¡¨ ---
    agent_image_data, agent_text_data = multimodal_agent_logic(
        saved_image_paths_for_agent[0] if saved_image_paths_for_agent[0] else None, # ä¼ é€’æ–°è·¯å¾„åˆ—è¡¨
        transcribed_text if transcribed_text else None
    )
    
    return agent_image_data, agent_text_data

if __name__ == "__main__":
    image_component_input = gr.Image(
        type="filepath",  # <--- é‡è¦æ›´æ”¹: Gradioå°†æä¾›æ–‡ä»¶è·¯å¾„
        label="ğŸ–¼ï¸ ä¸Šä¼ å›¾ç‰‡ (Upload Image) "
    )
    audio_component_input = gr.Audio(sources=["microphone", "upload"], type="filepath", label="ğŸ¤ è¯­éŸ³è¾“å…¥ (Speak or Upload Audio)")

    output_image_display = gr.Image(label="ğŸ–¼ï¸ Agent - å›¾åƒè¾“å‡º (Image Output)")
    output_text_display = gr.Textbox(label="ğŸ’¬ Agent - æ–‡æœ¬è¾“å‡º (Text Output)", lines=15, interactive=True)

    description = """
    è¿™æ˜¯ä¸€ä¸ªyuyæ¼”ç¤ºç•Œé¢ï¼š
    1.  ä¸Šä¼ å›¾ç‰‡ã€‚å›¾ç‰‡å°†è¢«ä¿å­˜åˆ°æœåŠ¡å™¨æœ¬åœ°çš„ `user_uploaded_images` æ–‡ä»¶å¤¹ã€‚
    2.  é€šè¿‡éº¦å…‹é£è¯´è¯æˆ–ä¸Šä¼ ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ã€‚
    3.  Agent å°†æ¥æ”¶ä¿å­˜åçš„å›¾ç‰‡è·¯å¾„å’Œè¯­éŸ³ï¼Œå¹¶æ ¹æ®æŒ‡ä»¤åŠ¨æ€è¾“å‡ºå›¾åƒæˆ–æ–‡å­—ã€‚
    """

    iface = gr.Interface(
        fn=process_inputs_for_dynamic_agent_output,
        inputs=[image_component_input, audio_component_input],
        outputs=[output_image_display, output_text_display],
        title="âœ¨ å¤šæ¨¡æ€ Agent âœ¨",
        description=description,
        allow_flagging="never",
        examples=[
            [None, None],
        ]
    )
    print(f"Gradio UI å³å°†å¯åŠ¨... ä¸Šä¼ çš„å›¾ç‰‡å°†ä¿å­˜åˆ° '{os.path.abspath(UPLOAD_IMAGE_DIR)}' æ–‡ä»¶å¤¹ã€‚")
    iface.launch()