该文档主要用于介绍如何使用llamafactory训练框架训练自己的模型

1. llamafactory环境配置
详细教程详见https://github.com/hiyouga/LLaMA-Factory

git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation

2. 数据集构造
pretrain类型数据集
pretrain类似数据用于训练基座模型，只需要构造单条文本即可训练，但是由于预训练需要大量数据和计算资源，只为了垂域任务应用一般不进行预训练。
'''json
[
    {
        "text": "A couple rediscover each other in their final moments.\nHey Catherine, read the script, lovely to read, I like this angle on the end of everything. My only departure is that I battle to see deserted supermarket, and empty streets, any how thats a deeper opinion on human nature.\nWell done it is an endearing sentimental story easy to identify with, with a dash of melancoly.\nAnd lastly, regarding the closing scene, I like your idea, and it is easy to write, and easy to read but that kinda stuff is difficult to communicate on film. Shots of books, papers, photos does not really communicating.\nSomething to think about, and again ... i could be wrong."
    }
]
'''


sft类型数据集
sft数据主要用于模型指令微调，可以收集1,2k条QA问答对训练模型在具体场景中的问答能力。
数据格式为
'''json
[
    {
    "instruction": "比较以下两位著名人物，使用特征列表并以句子形式提供输出。",
    "input": "人物1：阿尔伯特·爱因斯坦\n人物2：斯蒂芬·霍金\n特征：智力、教育、对世界的影响",
    "output": "阿尔伯特·爱因斯坦和斯蒂芬·霍金都是拥有极其出色智力的人物。两人都取得过非常高的教育成就，他们推进了科学发展并在世界范围内产生了深远的影响。爱因斯坦以其相对论和质能关系公式而闻名，而霍金以其关于黑洞和宇宙的发现而著称。两位科学家都以其深厚的学识和非凡的贡献影响了世界。"
  }
]
'''

强化学习RL数据集
强化学习主要用于人类偏好对齐（rlhf），LLM强化学习主要是套用了传统强化学习的概念（因为llm的推理方式是在生成的token上进行，类似强化学习的决策过程），强化学习需要针对同一问题的多个回答输出（采样出多个路径），让模型选择较优秀的输出。为什么不使用sft实现这个过程？因为sft的训练样本来自固定的数据集，存在过拟合风险，而强化学习的数据来自模型自身的输出（回答需要通过调整模型温度进行采样，手工或更好的llm对回答打分）在泛化上更好。
常用的强化学习技术包括dpo，ppo，grpo(deepseek实现，效果未能达成共识)。

dpo和ppo数据格式
dpo是一个低成本的强化学习方法，他使用排序模型把传统的rl训练目标转化成了一个有监督学习任务，优化稳定。然而dpo非常依赖数据量和质量，而且容易过拟合，适用于简单任务。数据上需要提问（"conversations"）、较优回答（"chosen"）、较差回答（"rejected"）三个部分构成，原则上回答应该来自训练的模型因为随机性产生的不同输出，但是实践上直接使用sft的标准答案似乎也有一定效果。
'''json
[
  {
    "conversations": [
      {
        "from": "human",
        "value": "国会的转发\n美国国会由众议院和参议院组成，每两年换届一次（参议员任期为6年，但参议院选举是错位的，使得国会的组成仍然每两年变化一次）。这两年期间按顺序标记，第115届国会发生在2017-2018年。\n\n密歇根大学信息学院的研究人员在这段时间内收集了现任国会议员（我们将“国会议员”缩写为MoC）的Twitter帖子，并对它们进行编码，标记为原创声明或其他用户提交的转发。我们将重点关注转发数据。这些发布的数据不仅包括转发的文本，还包括国会议员的信息和原始推文的帐户。\n#python:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport statsmodels.api as sm\nimport os\nfrom pandas.plotting import register\\_matplotlib\\_converters\nregister\\_matplotlib\\_converters()\n​\npd.options.mode.chained\\_assignment = None\n\n# 在接下来的内容中，我们将加载数据，但出于速度原因，我们将排除推文本身的文本。\n\ntweets = pd.read\\_csv(os.path.join(base, \"retweets\\_congress\\_115th.csv.gz\"), \n usecols = lambda x: x != \"full\\_text\",\n dtype = {\"tweet\\_id\\_str\": \"str\"},\n index\\_col = \"tweet\\_id\\_str\")\ntweets.shape\nout: (209856, 21)\n\ntweets.columns\nout:Index(['user\\_id\\_str', 'user\\_screen\\_name', 'user\\_followers\\_count', 'created\\_at',\n 'retweet\\_count', 'retweeted\\_status', 'year', 'bioguide', 'birthday',\n 'gender', 'name', 'state', 'chamber', 'party', 'region', 'rt\\_user\\_id',\n 'rt\\_screen\\_name', 'rt\\_followers\\_count', 'rt\\_bio', 'age', 'age\\_bucket'],\n dtype='object')\n\n第（a）部分\n国会主要由两个政党（民主党和共和党）组成，还有一些独立派。哪个政党负责大部分的转发？\n\n计算推文的政党的边际概率，并将结果保存到party\\_marg。显示这个结果。\n​\n第（b）部分\n我们可能会问的一个问题是，“国会成员彼此转发的频率是多少？”\n\n创建一个名为“is\\_congress\\_retweet”的新列，指示转发是否来自MoC。首先使用.unique()方法获取唯一的“user\\_id\\_str”值集合。然后使用这组id，在“rt\\_user\\_id”列上使用.isin(values)方法创建新的布尔列“is\\_congress\\_retweet”。\n\n找到转发MoC的边际概率，并将结果保存到“congress\\_retweet\\_marg”。显示这些结果。是更常见地转发国会议员还是来自国会外部？"
      }
    ],
    "chosen": {
      "from": "gpt",
      "value": "计算推文的政党边际概率，我们可以使用以下代码：\n```python\ntweets['party'] = tweets['party'].replace(0, 'Democrat')\ntweets['party'] = tweets['party'].replace(1, 'Republican')\ntweets['party'] = tweets['party'].replace(2, 'Independent')\n\nparty_counts = tweets.groupby('party').size().sort_values(ascending=False)\n\nparty_marg = party_counts / party_counts.sum()\n\nprint(\"政党的边际概率：\")\nprint(party_marg)\n```\n这段代码将'party'列中的数字值替换为相应的政党名称，然后按政党对推文进行分组并计算每个组的大小。最后，通过将每个组的大小除以总推文数来计算每个政党的边际概率。\n\n输出应该类似于：\n```markdown\n政党的边际概率：\n0.522152    民主党\n0.445541    共和党\n0.032307    独立派\n```\n这表明大多数转发来自民主党，其次是共和党，然后是独立派。\n\n要创建一个名为\"is\\_congress\\_retweet\"的新列，指示转发是否来自国会议员，我们可以使用以下代码：\n```python\ncongress_ids = tweets['user_id_str'].unique()\nis_congress_retweet = tweets['rt_user_id'].isin(congress_ids)\ntweets['is_congress_retweet'] = is_congress_retweet\n\ncongress_retweet_marg = tweets['is_congress_retweet'].mean()\nprint(\"转发国会议员的边际概率：\")\nprint(congress_retweet_marg)\n```\n这段代码首先获取唯一的\"user\\_id\\_str\"值集合，对应于国会议员的集合。然后，在\"rt\\_user\\_id\"列上使用.isin(values)方法创建一个新的布尔列\"is\\_congress\\_retweet\"，指示转发是否来自国会议员。最后，通过取\"is\\_congress\\_retweet\"列的平均值来计算转发国会议员的边际概率。\n\n输出应该类似于：\n```markdown\n转发国会议员的边际概率：\n0.775151\n```\n这表明大多数转发不是来自国会议员，而是来自国会之外。"
    },
    "rejected": {
      "from": "gpt",
      "value": "回答问题的第（a）部分，您可以通过首先使用groupby（）函数按政党对数据进行分组，然后计算每个组的转发数来计算政党的边际概率。具体来说，您可以使用以下代码：\n```scss\nparty_count = tweets.groupby('party').count()\nparty_count.loc['Democrats','retweet_count'] / party_count.loc['Republicans','retweet_count']\n```\n回答问题的第（b）部分，您可以创建一个新列来指示转发是否来自国会议员，方法是使用“bioguide”列上的.unique（）方法获取唯一的国会议员用户ID集合，然后使用该集合创建一个布尔列，指示转发是否来自国会议员，使用“rt_user_id”列上的.isin（）方法。最后，使用.sum（）方法来计算转发国会议员的边际概率，然后将其与转发来自国会以外的人的概率进行比较。以下是相关代码：\n```makefile\nimport pandas as pd\ntweets = pd.read_csv('retweets_congress_115th.csv')\n\n# 获取唯一的国会议员ID集合\nmovers_ids = tweets['bioguide'].unique()\n\n# 创建一个新的布尔列，指示转发是否来自国会议员\ntweets['is_congress_retweet'] = tweets['rt_user_id'].isin(movers_ids)\n\n# 计算转发国会议员或来自国会以外的人的边际概率\nmovers_retweets = tweets[tweets['is_congress_retweet'] == True].shape[0]\nnon_movers_retweets = tweets[tweets['is_congress_retweet'] == False].shape[0]\nproportion_movers_retweets = movers_retweets / (movers_retweets + non_movers_retweets)\nprint('Proportion of retweets from MoCs:', proportion_movers_retweets)\n```"
    }
  },
]
'''

dpo和ppo的一些区别和联系，能看到二者使用的数据格式上相同，最直观的区别是，dpo训练是只涉及待训练模型（action）和参考模型（ref）他俩本质上同一个模型，但是ref模型参数被冻结，训练开销大概是sft阶段的双倍（因为有两个模型），ppo训练涉及四个模型待训练模型（action）、参考模型（ref）（冻结）、奖励模型（reward）（冻结）、评价模型（critic），训练开销更大。而且ppo过程中训练样本主要用于训练reward模型，而后会对输出重新采样训练critic和action（ppo属于action-critic类强化学习方法），时长也更久，因此一般建议使用dpo，效果太差再考虑ppo。

构造好数据后需要在LLaMA-Factory/data/dataset_info.json中进行注册(注意：最好在LLaMa-Factory根目录下运行各类脚本，否则容易识别不到数据集等路径)

参考注册格式
"pt_test": {
    "file_name": "/root/autodl-tmp/datasets/mmlu_data/convert_data/all_pt/pt.json",
    "columns": {
      "prompt": "text"
    }
  },
"sft": {
    "file_name": "/root/autodl-tmp/datasets/mmlu_data/convert_data/all_pt/sft.json"
  },
"dpo-train": {
    "file_name": "/root/autodl-tmp/datasets/mmlu_data/convert_data/all_pt/rl_.json",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected"
    }
},

2. 模型训练（可以使用llamafactory-cli webui启动，使用可视化训练）
全部参数详情见/root/autodl-tmp/LLaMA-Factory/src/llamafactory/hparams/finetuning_args.py

pt训练脚本：/root/autodl-tmp/LLaMA-Factory/scripts/train_pt.sh
sft训练脚本 ：/root/autodl-tmp/LLaMA-Factory/scripts/train_sft.sh
dpo训练脚本：/root/autodl-tmp/LLaMA-Factory/scripts/train_dpo.sh
评估脚本：/root/autodl-tmp/LLaMA-Factory/scripts/eval.sh

基础参数介绍
--stage sft \用于控制训练的类型，评估时也使用sft, 常用可选项ppo, pt, dpo, rm(训练奖励模型的)
--do_train True \用于控制训练与否，评估时直接删掉这个配置项
--model_name_or_path /root/autodl-tmp/models/Qwen/Qwen2.5-VL-7B-Instruct \要训练的模型路径，这里应该填huggingface格式的模型
--finetuning_type lora \用于控制训练模式，有lora、full、freeze可选，full就是全参微调，lora不过多解释，freeze可以控制微调层数
                        如果在freeze模式下需要增加新的配置项 ---freeze_trainable_layers: 1，正数表述微调模型后几层，负数代表前几层
--template qwen2_vl \由于各家训练模型时候会有自己的系统提示词（system prompt），实测训练和微调时使用不同的模板可能会影响输出，比如deepseek系列模板会有<think>标签,具体模板可以查看LLaMA-Factory/src/llamafactory/data/template.py内容，不确定的话可以删掉此配置项，框架会自动解析模板
--dataset sft\LLaMA-Factory/data/dataset_info.json中注册的数据集名称
--num_train_epochs 5.0 \训练epoch数量，一般1到2个epoch就行，过多容易过拟合
--per_device_train_batch_size 2 \batchsize，如果显存不够需要调小
--output_dir saves/Qwen2.5-VL-7B-Instruct/lora/train_2025-04-3-13-29 \微调后模型权重的输出目录，对于lora方法而言，保存了适配器权重，
                                                                    lora方法相关的参数设置
                                                                    --lora_rank 8 \
                                                                    --lora_alpha 16 \
                                                                    --lora_dropout 0 \
                                                                    --lora_target all \这一项主要决定了lora的微调对象，/root/autodl-tmp/models/show_struct.py可以查看模型名称，将想微调的部分（nn.Linear）传入即可，比如--lora_target "model.layers.27.self_attn.v_proj,model.layers.27.self_attn.k_proj"

sft损失函数位置：LLaMA-Factory/src/llamafactory/train/sft/trainer.py，该框架使用的是transformer库Trainer类内置的损失函数，
@dataclass
class LabelSmoother:
    """
    Adds label-smoothing on a pre-computed output from a Transformers model.

    Args:
        epsilon (`float`, *optional*, defaults to 0.1):
            The label smoothing factor.
        ignore_index (`int`, *optional*, defaults to -100):
            The index in the labels to ignore when computing the loss.
    """

    epsilon: float = 0.1
    ignore_index: int = -100

    def __call__(self, model_output, labels, shift_labels=False):
        logits = model_output["logits"] if isinstance(model_output, dict) else model_output[0]
        if shift_labels:
            logits = logits[..., :-1, :].contiguous()
            labels = labels[..., 1:].contiguous()

        log_probs = -nn.functional.log_softmax(logits, dim=-1)
        if labels.dim() == log_probs.dim() - 1:
            labels = labels.unsqueeze(-1)

        padding_mask = labels.eq(self.ignore_index)
        # In case the ignore_index is -100, the gather will fail, so we replace labels by 0. The padding_mask
        # will ignore them in any case.
        labels = torch.clamp(labels, min=0)
        nll_loss = log_probs.gather(dim=-1, index=labels)
        # works for fp16 input tensor too, by internally upcasting it to fp32
        smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)

        nll_loss.masked_fill_(padding_mask, 0.0)
        smoothed_loss.masked_fill_(padding_mask, 0.0)

        # Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):
        num_active_elements = padding_mask.numel() - padding_mask.long().sum()
        nll_loss = nll_loss.sum() / num_active_elements
        smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])
        return (1 - self.epsilon) * nll_loss + self.epsilon * smoothed_loss
如果需要自定义损失函数，比如针对不同的词给出不同权重，可以在trainer.py中传入特定损失函数。

训练后合并模型导出


使用llamafactory提供的FASTAPI进行部署推理，见/root/autodl-tmp/LLaMA-Factory/scripts/api.sh
llamafactory-cli api\
    --model_name_or_path /root/autodl-tmp/models/deepseek-ai/deepseek-coder-1.3b-base
可以查看http://localhost:8000/docs 中api对话文档格式
如果使用ollama需要把模型权重从huggingface格式转化成gguf，但是llamafactory本身提供api服务且不需要按照，暂时不考虑ollama



